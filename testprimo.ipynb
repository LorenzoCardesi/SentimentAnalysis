{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-xl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/SentimentAnalysis/.llm_eval/lib/python3.12/site-packages/transformers/modeling_utils.py:3838\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3829\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3831\u001b[0m     (\n\u001b[1;32m   3832\u001b[0m         model,\n\u001b[1;32m   3833\u001b[0m         missing_keys,\n\u001b[1;32m   3834\u001b[0m         unexpected_keys,\n\u001b[1;32m   3835\u001b[0m         mismatched_keys,\n\u001b[1;32m   3836\u001b[0m         offload_index,\n\u001b[1;32m   3837\u001b[0m         error_msgs,\n\u001b[0;32m-> 3838\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3846\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3849\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3858\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/SentimentAnalysis/.llm_eval/lib/python3.12/site-packages/transformers/modeling_utils.py:4125\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4123\u001b[0m             model\u001b[38;5;241m.\u001b[39mapply(model\u001b[38;5;241m.\u001b[39m_initialize_weights)\n\u001b[1;32m   4124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4125\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4127\u001b[0m \u001b[38;5;66;03m# Set some modules to fp32 if any\u001b[39;00m\n\u001b[1;32m   4128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_in_fp32_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SentimentAnalysis/.llm_eval/lib/python3.12/site-packages/torch/nn/modules/module.py:895\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    894\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m--> 895\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/SentimentAnalysis/.llm_eval/lib/python3.12/site-packages/transformers/modeling_utils.py:1744\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/SentimentAnalysis/.llm_eval/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:821\u001b[0m, in \u001b[0;36mT5PreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    814\u001b[0m     module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    816\u001b[0m     module,\n\u001b[1;32m    817\u001b[0m     (T5Model, T5ForConditionalGeneration, T5EncoderModel, T5ForQuestionAnswering),\n\u001b[1;32m    818\u001b[0m ):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;66;03m# Mesh TensorFlow embeddings initialization\u001b[39;00m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[1;32m    823\u001b[0m         module\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnormal_(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, std\u001b[38;5;241m=\u001b[39mfactor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\").to(\"cuda\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    dataset.rename(columns={'sentiment': 'Sentiment', 'review': 'Text'}, inplace=True)\n",
    "    dataset['Sentiment'] = dataset['Sentiment'].replace({'negative': 0, 'positive': 1})\n",
    "    dataset['Sentiment'] = dataset['Sentiment'].replace({'Negative': 0, 'Positive': 1})\n",
    "    dataset = dataset.sample(frac=1, random_state=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66631/2921636663.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sentiment'] = dataset['Sentiment'].replace({'Negative': 0, 'Positive': 1})\n"
     ]
    }
   ],
   "source": [
    "original = pd.read_csv(\"data/test_original.tsv\", sep='\\t')\n",
    "preprocess(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66631/2921636663.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sentiment'] = dataset['Sentiment'].replace({'Negative': 0, 'Positive': 1})\n"
     ]
    }
   ],
   "source": [
    "contrast = pd.read_csv(\"data/test_contrast.tsv\", sep='\\t')\n",
    "preprocess(contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(*, prompt, sentence, tokenizer, model):\n",
    "    inputs = tokenizer(prompt(sentence),  padding='max_length', return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    \n",
    "    if \"negative\" in result.lower():\n",
    "        return 0\n",
    "    elif \"positive\" in result.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A wonderful early musical film from Rene Clair, as fun and witty as his silent \"The Italian Straw Hat\". Using sound in a expressive way and not just for dialogue and effects, Clair influenced early musicals in America (the opera scene from A Night at the Opera is strongly influenced by Le Million, for example). Should (but won't) be seen by all cinephiles, and the DVD from Criterion is exactly as good as you'd expect. There's not a ton of extras, but most DVD extras I've seen are useless fluff, and the Clair interview on disc is one I hadn't ever seen. Get it while it's still around.\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Melissa's sixteenth birthday is right around the corner and she's just discovering her sexuality with boys. But it turns out that all the guys that she spends time with all wind up murdered in this generic '80's slasher film. It's up to the local town sheriff Dan Burke (Bo Hopkins, The Wild Bunch) and his annoying mystery-loving goody two-shoes daughter, Marci (Dana Kimmell, Friday the 13th part 3), to get to the bottom of these killings.<br /><br />This film focuses more on the mystery and melodrama aspects of the movie and less on the killings themselves and thus is able to differentiate itself from a lot of it's '80's Slasher brethren. It doesn't hurt that Alesia has a great body (I feel the need to stress the obvious with stating that the actress is over 18 and thus convey that i'm not overly perverted). On the downside, the movie is hampered with a few plot points that are underdeveloped and unnecessary, a grating theme some that is used a bit too often, and an ending that is a tad anti-climatic. But the good outweigh the bad (barely). Give this a rent, but I wouldn't buy it.<br /><br />Eye Candy: Aleisa Shirley shows her tits, bush and ass <br /><br />My Grade: C <br /><br />Code Red DVD Extras: An intro by star Aleisa Shirley and Director of Intruder, Scott Spiegel; Both Director's cut & theatrical version of the film; Audio conversation with star Shirley and Director Jim Sotos; interview with Shirley, Sotos & Bo Hopkins; still gallery; theatrical trailer for this film; and trailers for Nightmare, Stunt Rock, Rituals, & Balalaika Conspiracy\n",
      "Sentiment: negative\n",
      "\n",
      "Text: A spoiler.<br /><br />What three words can guarantee you a terrible film? Cheap Canadian Production. THE BRAIN fits those words perfectly. Terrible script, idiotic acting and hilarious special effects make this a must for every BAD movie fan. The horror is hilarious. The post production team looks like it gave up. What makes THE BRAIN admirable is in the second half, it actually tries to be good! Can a bit of ingenuity and consistency save what is already a joke?<br /><br />It's around Christmas time. A mother and daughter are murdered by one of the funniest looking villains ever. The day later, a rebel teen gets into enough trouble that he is sent for a psychiatric analysis.<br /><br />If a cop 's head is chopped off and a stranger with blood on him and a bloody axe told you some kids did it, who would you believe? What begins as funny turns dull and tiring toward the end when THE BRAIN tries to be serious. A child cannot be frightened by the scary moments. THE BRAIN is too funny a concept to try and be gritty. The Psychological Research Institute is larger than major manufacturing plants! Our ugly villain and its cohorts get credit for pulling some of the worst acting I have seen. Viewer discretion advised heavily.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: This movie was so bad that my i.q. went down about 40 points after seeing it. It made me wonder who could sit through the weeks it took to make it and think that it was worth it. It must of been some kind of personal favor to Van Damme.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: My Sentence\n",
      "Sentiment: \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def build_example(dataset, index):\n",
    "    return (\"Text: \" + dataset.Text[index] + \"\\nSentiment: \" + [\"negative\",\"positive\"][dataset.Sentiment[index]] + \"\\n\\n\")\n",
    "\n",
    "def get_build_prompt(dataset, index, shots, system):\n",
    "    def build_prompt(sent):\n",
    "        ret = \"\"\n",
    "        if(system):\n",
    "            if(shots > 0):\n",
    "                ret += \"Classify the text into negative or positive. Here you have some examples:\\n\\n\"\n",
    "            else:\n",
    "                ret += \"Classify the text into negative or positive.\\n\\n\"\n",
    "        help_list = []\n",
    "        for j in range(shots):\n",
    "            help = random.sample(range(len(dataset)), 1)[0]\n",
    "            while(index == help or help in help_list):\n",
    "                help = random.sample(range(len(dataset)), 1)[0]\n",
    "            help_list.append(help)\n",
    "            example = build_example(dataset, help)\n",
    "            ret += example\n",
    "        ret += \"Text: \" + sent + \"\\nSentiment: \"\n",
    "        return ret\n",
    "    return build_prompt\n",
    "\n",
    "example = get_build_prompt(original, 1, shots=4, system=False)\n",
    "print(example(\"My Sentence\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(dataset, system):\n",
    "    overall_results = dict()\n",
    "\n",
    "    for shots in [0,1,2,3]:\n",
    "        if(shots == 0):\n",
    "             test = 1\n",
    "        else:\n",
    "            test = 3\n",
    "        print(\"Running shot\", shots)\n",
    "        for run in range(test):\n",
    "            print(\"Run\", run)\n",
    "            targets = []\n",
    "            predictions = []\n",
    "            total = len(dataset)\n",
    "            \n",
    "            for i in tqdm(range(total), total=total):\n",
    "                prompt = get_build_prompt(dataset, i, shots, system)\n",
    "                targets.append(dataset.Sentiment[i])\n",
    "                predictions.append(inference(prompt=prompt, sentence=dataset.Text[i], tokenizer=tokenizer, model=model))\n",
    "            \n",
    "            right, wrong, no_sentiment = 0, 0, 0\n",
    "            \n",
    "            for j in range(len(predictions)):\n",
    "                if(predictions[j] == 2):\n",
    "                    no_sentiment += 1\n",
    "                elif(targets[j] == predictions[j]):\n",
    "                        right += 1\n",
    "                elif(targets[j] != predictions[j]):\n",
    "                        wrong += 1\n",
    "\n",
    "            if shots not in overall_results:\n",
    "                overall_results[shots] = dict()\n",
    "\n",
    "            overall_results[shots][run] = {'correct' : right/len(predictions), 'wrong' : wrong/len(predictions), 'no_sentiment': no_sentiment/len(predictions)}\n",
    "            # report = classification_report(targets, predictions, labels=[0,1,2], output_dict=True, zero_division=0)\n",
    "            # overall_results[shots] = report\n",
    "\n",
    "    return overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def write_csv(file_path, dataframe):\n",
    "    csv_path = \"csv/\" + file_path + \".csv\"\n",
    "    json_path = \"json/\" + file_path + \".json\"\n",
    "    with open(csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Scrittura dell'intestazione\n",
    "        writer.writerow(['', '', file_path.replace(\"_results\", \"\"), ''])\n",
    "        writer.writerow(['', '', 'Errors', '', '', 'Errors', '', '', 'Errors', '', '', 'Errors'])\n",
    "        writer.writerow(['Runs', 'Correct', 'Wrong', 'No_sentiment', 'Correct', 'Wrong', 'No_sentiment', 'Correct', 'Wrong', 'No_sentiment', 'Correct', 'Wrong', 'No_sentiment'])\n",
    "        \n",
    "        # Scrittura dei dati\n",
    "        for j in range(3):\n",
    "            field = [\"Run \" + str(j)]\n",
    "            if(j != 0):\n",
    "                field.append(\"\")\n",
    "                field.append(\"\")\n",
    "                field.append(\"\")\n",
    "            for shots, runs in dataframe.items():\n",
    "                for run_number, metrics in runs.items():\n",
    "                    if(run_number == j):\n",
    "                        field.append(metrics['correct'])\n",
    "                        field.append(metrics['wrong'])\n",
    "                        field.append(metrics['no_sentiment'])\n",
    "            writer.writerow(field)\n",
    "    print(f\"I dati sono stati scritti su {csv_path}.\")\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataframe, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 0\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:12<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 1\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:16<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:17<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:17<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 2\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:42<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:43<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:41<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 3\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:16<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:12<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:14<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dati sono stati scritti su csv/seed_test.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = eval(original, system=False)\n",
    "\n",
    "file_name = \"seed_test\"\n",
    "\n",
    "write_csv(file_name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    correct  no_sentiment     wrong\n",
      "0  0.954918      0.008197  0.036885\n",
      "1  0.960383      0.000000  0.039617\n",
      "2  0.963115      0.000000  0.036885\n",
      "3  0.963115      0.000000  0.036885\n"
     ]
    }
   ],
   "source": [
    "averages = {}\n",
    "\n",
    "for shot, runs in test.items():\n",
    "    correct_total = 0\n",
    "    no_sentiment_total = 0\n",
    "    wrong_total = 0\n",
    "    run_count = len(runs)\n",
    "    \n",
    "    for run_id, values in runs.items():\n",
    "        correct_total += values['correct']\n",
    "        no_sentiment_total += values['no_sentiment']\n",
    "        wrong_total += values['wrong']\n",
    "    \n",
    "    averages[shot] = {\n",
    "        'correct': correct_total / run_count,\n",
    "        'no_sentiment': no_sentiment_total / run_count,\n",
    "        'wrong': wrong_total / run_count\n",
    "    }\n",
    "\n",
    "# Creazione di un dataframe per visualizzare i risultati\n",
    "averages_df = pd.DataFrame.from_dict(averages, orient='index')\n",
    "print(averages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shot  run   correct\n",
      "0     0    0  0.954918\n",
      "1     1    0  0.963115\n",
      "2     1    1  0.959016\n",
      "3     1    2  0.959016\n",
      "4     2    0  0.963115\n",
      "5     2    1  0.965164\n",
      "6     2    2  0.961066\n",
      "7     3    0  0.961066\n",
      "8     3    1  0.963115\n",
      "9     3    2  0.965164\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "for shot, runs in test.items():\n",
    "    for run_id, values in runs.items():\n",
    "        accuracies.append({\n",
    "            'shot': shot,\n",
    "            'run': run_id,\n",
    "            'correct': values['correct']\n",
    "        })\n",
    "\n",
    "# Creazione di un DataFrame per visualizzare i risultati\n",
    "accuracies_df = pd.DataFrame(accuracies)\n",
    "print(accuracies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shot, runs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_id, values \u001b[38;5;129;01min\u001b[39;00m runs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m         accuracies\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshot\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(shot),\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(run_id),\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m: values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m         })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "for shot, runs in test.items():\n",
    "    for run_id, values in runs.items():\n",
    "        accuracies.append({\n",
    "            'shot': int(shot),\n",
    "            'run': int(run_id),\n",
    "            'correct': values['correct']\n",
    "        })\n",
    "\n",
    "# Creazione di un DataFrame\n",
    "accuracies_df = pd.DataFrame(accuracies)\n",
    "\n",
    "# Creare una griglia completa di combinazioni di shot e run\n",
    "all_shots = accuracies_df['shot'].unique()\n",
    "all_runs = accuracies_df['run'].unique()\n",
    "complete_index = pd.MultiIndex.from_product([all_shots, all_runs], names=['shot', 'run'])\n",
    "\n",
    "# Riorganizzare il DataFrame per avere dati completi con NaN dove mancano i dati\n",
    "complete_accuracies_df = accuracies_df.set_index(['shot', 'run']).reindex(complete_index).reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Definizione della larghezza delle barre\n",
    "bar_width = 0.2\n",
    "\n",
    "# Generazione delle posizioni sull'asse x per ciascuna run\n",
    "x_pos = np.arange(len(all_shots))\n",
    "\n",
    "# Plot di ogni run\n",
    "for i, run in enumerate(all_runs):\n",
    "    run_data = complete_accuracies_df[complete_accuracies_df['run'] == run]\n",
    "    bars = plt.bar(x_pos + i * bar_width, run_data['correct'], width=bar_width, label=f'Run {run}')\n",
    "    \n",
    "    # Aggiunta di etichette per i valori sopra ogni barra\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.0005, f'{yval:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Configurazione degli assi e delle etichette\n",
    "plt.xlabel('Shot')\n",
    "plt.ylabel('Correct Accuracy')\n",
    "plt.title('Accuracy per Shot and Run')\n",
    "plt.xticks(x_pos + bar_width * (len(all_runs) - 1) / 2, [f'Shot {shot}' for shot in all_shots])\n",
    "plt.ylim(0.94, 0.97)  # Zoom sull'intervallo di valori di interesse\n",
    "plt.legend(title='Runs')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: paplay: not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"paplay complete.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
