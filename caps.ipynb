{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cardesi/SentimentAnalysis/.llm_eval/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\").to(\"cuda\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    dataset.rename(columns={'sentiment': 'Sentiment', 'review': 'Text'}, inplace=True)\n",
    "    dataset['Sentiment'] = dataset['Sentiment'].replace({'negative': 0, 'positive': 1})\n",
    "    dataset['Sentiment'] = dataset['Sentiment'].replace({'Negative': 0, 'Positive': 1})\n",
    "    dataset = dataset.sample(frac=1, random_state=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106429/2921636663.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sentiment'] = dataset['Sentiment'].replace({'Negative': 0, 'Positive': 1})\n"
     ]
    }
   ],
   "source": [
    "original = pd.read_csv(\"data/test_original.tsv\", sep='\\t')\n",
    "preprocess(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106429/2921636663.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset['Sentiment'] = dataset['Sentiment'].replace({'Negative': 0, 'Positive': 1})\n"
     ]
    }
   ],
   "source": [
    "contrast = pd.read_csv(\"data/test_contrast.tsv\", sep='\\t')\n",
    "preprocess(contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(*, prompt, sentence, tokenizer, model):\n",
    "    inputs = tokenizer(prompt(sentence),  padding='max_length', return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    \n",
    "    if \"negative\" in result.lower():\n",
    "        return 0\n",
    "    elif \"positive\" in result.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Never heard of this movie,saw it on DVD.Great movie,perfect example of a movie that took every cast member to make it work.No overhyped typical Hollywood movie with the same old overhyped actors.No current Quote \"A\" list actor could have pulled off any performance in this movie.Brought back memories of my own post Vietnam war military experiences.It concentrated on the people who were sent to fight.As was portrayed by the characters who had fears and emotions even if some volunteered for service.They were regular people too,some just weren't cut out for military life,I remember a few in my experience--to put it mildly couldn't adapt to military life either-but I'll never forget them-should have stayed in touch.I highly recommend it and then think about those serving present day in Afganistan.Basic training is a trip, notice those drill sergeants aren't morning people and maybe they need \"sensitivity training\" HA!HA!HA!\n",
      "SENTIMENT: POSITIVE\n",
      "\n",
      "TEXT: Very businesslike authority with little responsibility and only a desire to keep his/her name clean - check. A veteran cop that has bad relationship with his family - check. Mafia guys that while criminals, want to do something good vigilante style - check. A sociopath and loyal mafia guy not hesitant to kill people to make an example - check. Cops' methods being less effective than the mafia guy's brutal yet very effective methods - check. A corrupt cop tying the authority, the criminals and the police together - check.<br /><br />Slow motion and/or jerky frame rates for showing what the actor's reaction can't - check. A serial killer whose background is explained in far too much detail, esp. using childhood abuse as the reason for everything - check. A child spree killer that is very, very non-menacing - check. Foreshadowing of the veteran cop's moral values not being what the killer deserves in the movie's and the majority of characters' opinion - check. Morally ambiguous and predictable ending thanks to the foreshadowing and the good veteran cop's coming to terms he should submit to the vigilante attitude of the majority of the characters - check.<br /><br />Recently saw this on TV and decided to endure it because it had Dennis Hopper in it and I could not sleep - check. Realized that was a mistake and should just have stared at the ceiling - check.\n",
      "SENTIMENT: NEGATIVE\n",
      "\n",
      "TEXT: As a premise, this backwoods version of the Dead Calm storyline had promise.<br /><br />However, director Eric Red's inability to render a convincing hurricane leads to a deluge of continuity and lighting errors.<br /><br />Ultimately, the viewer is more spellbound by the bizarre weather effects than the intended storyline. Intermittent spates of ham-fisted over-direction are similarly distracting.<br /><br />Charles Dance, doing an 'inbred backwoods hardass' schtick, does his best to save the movie. But ultimately, Undertow squeals like a pig ... and has more ham to boot.\n",
      "SENTIMENT: NEGATIVE\n",
      "\n",
      "TEXT: This must me one of the worst takes on vampires ever conceived by men. How can one turn such a mesmerizing subject into a totally uninspiring story? Apparantly not such a difficult task... First of all, a conditio sine qua non of any vampirefilm is a dark and gloomy atmosphere with a nice sexy touch, this one lacks all these things.. Too much light - the spots! oh my god, why in the name of Christ/Judas was that about?<br /><br />Every time Dracula came about he was devoured by light (in the script to keep him weak, for the record: just weak) There was only one scene that made it almost worth watching, near the ending of the movie (beatiful dancingscene with Dracula and his new conquest). I really enjoyed the first one, the Judas-twist was defintely original, but this one's just not good, not in any way. Hopefully the third one will cary the vampire-signature I like so much in other classics like Herzog's Nosferatu, Coppola's Dracula or even Interview with the vampire.\n",
      "SENTIMENT: NEGATIVE\n",
      "\n",
      "TEXT: My Sentence\n",
      "SENTIMENT: \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def build_example(dataset, index, caps, spaced):\n",
    "    string = \"\"\n",
    "    if(caps == \"all\"):\n",
    "        string = \"TEXT: \" + dataset.Text[index] + \"\\nSENTIMENT: \" + [\"NEGATIVE\",\"POSITIVE\"][dataset.Sentiment[index]] + \"\\n\\n\"\n",
    "    elif(caps == \"key\"):\n",
    "        string = \"TEXT: \" + dataset.Text[index] + \"\\nSENTIMENT: \" + [\"negative\",\"positive\"][dataset.Sentiment[index]] + \"\\n\\n\"\n",
    "    elif(caps == \"lower\"):\n",
    "        string = \"Text: \" + dataset.Text[index] + \"\\nSentiment: \" + [\"negative\",\"positive\"][dataset.Sentiment[index]] + \"\\n\\n\"\n",
    "    return string\n",
    "\n",
    "def get_build_prompt(dataset, index, shots, system, caps, spaced):\n",
    "    def build_prompt(sent):\n",
    "        ret = \"\"\n",
    "        if(system):\n",
    "            if(shots > 0):\n",
    "                ret += \"Classify the text into negative or positive. Here you have some examples:\\n\\n\"\n",
    "            else:\n",
    "                ret += \"Classify the text into negative or positive.\\n\\n\"\n",
    "        help_list = []\n",
    "        for j in range(shots):\n",
    "            help = random.sample(range(len(dataset)), 1)[0]\n",
    "            while(index == help or help in help_list):\n",
    "                help = random.sample(range(len(dataset)), 1)[0]\n",
    "            help_list.append(help)\n",
    "            example = build_example(dataset, help, caps, spaced)\n",
    "            ret += example\n",
    "        if(caps==\"all\" or caps==\"key\"):\n",
    "            ret += \"TEXT: \" + sent + \"\\nSENTIMENT: \"\n",
    "        elif(caps==\"lower\"):\n",
    "            ret += \"Text: \" + sent + \"\\nSentiment: \"\n",
    "        return ret\n",
    "    return build_prompt\n",
    "\n",
    "example = get_build_prompt(original, 1, shots=4, system=False, caps=\"all\", spaced=False)\n",
    "print(example(\"My Sentence\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(dataset, system, overall_results, caps, spaced):\n",
    "    # overall_results = dict()\n",
    "\n",
    "    for shots in [0,1,2,3]:\n",
    "        if(shots == 0):\n",
    "             test = 1\n",
    "        else:\n",
    "            test = 1\n",
    "        print(\"Running shot\", shots)\n",
    "        for run in range(test):\n",
    "            print(\"Run\", run)\n",
    "            targets = []\n",
    "            predictions = []\n",
    "            total = len(dataset)\n",
    "            \n",
    "            for i in tqdm(range(total), total=total):\n",
    "                prompt = get_build_prompt(dataset, i, shots, system, caps, spaced)\n",
    "                targets.append(dataset.Sentiment[i])\n",
    "                predictions.append(inference(prompt=prompt, sentence=dataset.Text[i], tokenizer=tokenizer, model=model))\n",
    "            \n",
    "            right, wrong, no_sentiment = 0, 0, 0\n",
    "            \n",
    "            for j in range(len(predictions)):\n",
    "                if(predictions[j] == 2):\n",
    "                    no_sentiment += 1\n",
    "                elif(targets[j] == predictions[j]):\n",
    "                        right += 1\n",
    "                elif(targets[j] != predictions[j]):\n",
    "                        wrong += 1\n",
    "\n",
    "            if shots not in overall_results:\n",
    "                overall_results[shots] = dict()\n",
    "\n",
    "            overall_results[shots][caps] = {'correct' : right/len(predictions), 'wrong' : wrong/len(predictions), 'no_sentiment': no_sentiment/len(predictions)}\n",
    "            # report = classification_report(targets, predictions, labels=[0,1,2], output_dict=True, zero_division=0)\n",
    "            # overall_results[shots] = report\n",
    "\n",
    "    return overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 0\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:04<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 1\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:08<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 2\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:30<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 3\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [03:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 0\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:03<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 1\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:18<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 2\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:44<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 3\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:18<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 0\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:12<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 1\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:17<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 2\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [01:43<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running shot 3\n",
      "Run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [02:15<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "overall_results = dict()\n",
    "test = eval(original, system=False, overall_results=overall_results, caps=\"all\", spaced=False)\n",
    "test = eval(original, system=False, overall_results=overall_results, caps=\"key\", spaced=False)\n",
    "test = eval(original, system=False, overall_results=overall_results, caps=\"lower\", spaced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def write_csv(file_path, dataframe):\n",
    "    csv_path = \"csv/\" + file_path + \".csv\"\n",
    "    json_path = \"json/\" + file_path + \".json\"\n",
    "    with open(csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Scrittura dell'intestazione\n",
    "        writer.writerow(['', '', file_path.replace(\"_results\", \"\"), ''])\n",
    "        writer.writerow(['', '', 'Errors', '', '', 'Errors', '', '', 'Errors', '', '', 'Errors'])\n",
    "        writer.writerow(['Runs', 'Correct', 'Wrong', 'No_sentiment', 'Correct', 'Wrong', 'No_sentiment', 'Correct', 'Wrong', 'No_sentiment', 'Correct', 'Wrong', 'No_sentiment'])\n",
    "        \n",
    "        # Scrittura dei dati\n",
    "        for j in range(3):\n",
    "            field = [\"Run \" + str(j)]\n",
    "            if(j != 0):\n",
    "                field.append(\"\")\n",
    "                field.append(\"\")\n",
    "                field.append(\"\")\n",
    "            for shots, runs in dataframe.items():\n",
    "                for run_number, metrics in runs.items():\n",
    "                    if(run_number == j):\n",
    "                        field.append(metrics['correct'])\n",
    "                        field.append(metrics['wrong'])\n",
    "                        field.append(metrics['no_sentiment'])\n",
    "            writer.writerow(field)\n",
    "    print(f\"I dati sono stati scritti su {csv_path}.\")\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataframe, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dati sono stati scritti su csv/caps.csv.\n"
     ]
    }
   ],
   "source": [
    "file_name = \"caps\"\n",
    "\n",
    "write_csv(file_name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    correct  no_sentiment     wrong\n",
      "0  0.856557      0.106557  0.036885\n",
      "1  0.836066      0.104508  0.059426\n",
      "2  0.884563      0.050546  0.064891\n",
      "3  0.910519      0.025956  0.063525\n"
     ]
    }
   ],
   "source": [
    "averages = {}\n",
    "\n",
    "for shot, runs in test.items():\n",
    "    correct_total = 0\n",
    "    no_sentiment_total = 0\n",
    "    wrong_total = 0\n",
    "    run_count = len(runs)\n",
    "    \n",
    "    for run_id, values in runs.items():\n",
    "        correct_total += values['correct']\n",
    "        no_sentiment_total += values['no_sentiment']\n",
    "        wrong_total += values['wrong']\n",
    "    \n",
    "    averages[shot] = {\n",
    "        'correct': correct_total / run_count,\n",
    "        'no_sentiment': no_sentiment_total / run_count,\n",
    "        'wrong': wrong_total / run_count\n",
    "    }\n",
    "\n",
    "# Creazione di un dataframe per visualizzare i risultati\n",
    "averages_df = pd.DataFrame.from_dict(averages, orient='index')\n",
    "print(averages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    shot    run   correct\n",
      "0      0    all  0.807377\n",
      "1      0    key  0.807377\n",
      "2      0  lower  0.954918\n",
      "3      1    all  0.584016\n",
      "4      1    key  0.963115\n",
      "5      1  lower  0.961066\n",
      "6      2    all  0.723361\n",
      "7      2    key  0.965164\n",
      "8      2  lower  0.965164\n",
      "9      3    all  0.809426\n",
      "10     3    key  0.961066\n",
      "11     3  lower  0.961066\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "for shot, runs in test.items():\n",
    "    for run_id, values in runs.items():\n",
    "        accuracies.append({\n",
    "            'shot': shot,\n",
    "            'run': run_id,\n",
    "            'correct': values['correct']\n",
    "        })\n",
    "\n",
    "# Creazione di un DataFrame per visualizzare i risultati\n",
    "accuracies_df = pd.DataFrame(accuracies)\n",
    "print(accuracies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shot, runs \u001b[38;5;129;01min\u001b[39;00m test\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_id, values \u001b[38;5;129;01min\u001b[39;00m runs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m         accuracies\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshot\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(shot),\n\u001b[0;32m---> 11\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m: values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m         })\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Creazione di un DataFrame\u001b[39;00m\n\u001b[1;32m     16\u001b[0m accuracies_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(accuracies)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'all'"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "colors = {\n",
    "    'all': '#1f77b4',    # blu\n",
    "    'key': '#ff7f0e',    # arancione\n",
    "    'lower': '#2ca02c'   # verde\n",
    "}\n",
    "\n",
    "# Impostazione della larghezza delle barre e della posizione x\n",
    "bar_width = 0.25\n",
    "shots = accuracies_df['shot'].unique()\n",
    "x_pos = np.arange(len(shots))\n",
    "\n",
    "# Creare il grafico a barre\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, run in enumerate(accuracies_df['run'].unique()):\n",
    "    # Filtrare i dati per 'run'\n",
    "    run_data = accuracies_df[accuracies_df['run'] == run]\n",
    "    # Creare le barre\n",
    "    bars = plt.bar(x_pos + i * bar_width, run_data['correct'], color=colors[run], width=bar_width, label=f'Run {run}')\n",
    "    \n",
    "    # Aggiungere etichette di valore sopra ogni barra\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.005, f'{yval:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Configurare l'asse x e y\n",
    "plt.xlabel('Shot')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Shot and Run')\n",
    "plt.xticks(x_pos + bar_width, [f'Shot {int(shot)}' for shot in shots])\n",
    "plt.ylim(0.5, 1.0)  # Definire i limiti dell'asse y per migliorare la visualizzazione\n",
    "\n",
    "# Aggiungere una griglia per una migliore leggibilità\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Aggiungere la legenda\n",
    "plt.legend(title='Runs')\n",
    "\n",
    "# Visualizzare il grafico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"paplay complete.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
